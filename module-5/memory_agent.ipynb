{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Agent\n",
    "\n",
    "## Review\n",
    "\n",
    "We created a chatbot that saves semantic memories to a single [user profile](https://langchain-ai.github.io/langgraph/concepts/memory/#profile) or [collection](https://langchain-ai.github.io/langgraph/concepts/memory/#collection).\n",
    "\n",
    "We introduced [Trustcall](https://github.com/hinthornw/trustcall) as a way to update either schema.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going to turn our chatbot into a simple [agent](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/).\n",
    "\n",
    "The chatbot *always* reflected on the conversation and saved memories. \n",
    "\n",
    "The central difference is that the agent will decide *when* to save memories. \n",
    "\n",
    "For this, we're going to introduce a tool that allows the agent to save memories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Check if the variable is set in the OS environment\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # If not set, prompt the user for input\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "    \n",
    "    # Set the environment variable for the current process\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a router\n",
    "\n",
    "There are many different [agent](https://langchain-ai.github.io/langgraph/concepts/high_level/) architectures to choose from.\n",
    "\n",
    "Here, we'll implement something simple: a router that decides when to save memories.\n",
    "\n",
    "This will reflect on the chat history and any prior memories to decide whether to update the memory collection.\n",
    "\n",
    "The updating itself will be handled by `Trustcall`, as before!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Schema for binary decision to save memories\n",
    "class SaveMemory(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    store_memories: bool = Field(description=\"Decision to save memories based on the conversation with the user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's chat model [chat model](https://python.langchain.com/docs/concepts/chat_models/) interface has a [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) method to enforce structured output.\n",
    "\n",
    "As we showed before, this is useful when we want to enforce that the output conforms to a schema, and it parses the output for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Bind the schema to the model\n",
    "model_with_structure = model.with_structured_output(SaveMemory)\n",
    "\n",
    "# Current memory collection\n",
    "info = \"This user's name is Lance and he likes to bike.\"\n",
    "\n",
    "# System message\n",
    "system_msg = f\"\"\"You manage are deciding whether to update the memory collection for the user.\n",
    "    Here is the current memory collection (it may be empty): <memories>{info}</memories>\n",
    "    Here is the chat history. Assess whether the chat history contains any information that should be added to the memory collection.\"\"\"\n",
    "\n",
    "# Invoke with new information\n",
    "store_memories_flag = model_with_structure.invoke([SystemMessage(content=system_msg)]+[HumanMessage(content=\"I like to eat croissants\")])\n",
    "store_memories_flag.store_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph definition \n",
    "\n",
    "We add a simple router, `route_message`, that makes a binary decision to save memories.\n",
    "\n",
    "The memory collection updating is handled by `Trustcall` in the `write_memory` node, as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from trustcall import create_extractor\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import ToolMessage, AnyMessage\n",
    "from langchain_core.messages import merge_message_runs\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "    \n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Schema \n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "    context: str = Field(description=\"Additional context for the memory. For example: This was mentioned while discussing career options in Europe.\")\n",
    "\n",
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    # This allows the extractor to insert new memories\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "# Node definitions\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"[{mem.key}]: {mem.value}\" for mem in memories)\n",
    "    system_msg = f\"\"\"You are a helpful assistant with memory that provides information about the user.  \n",
    "    If you have memory for this user, use it to personalize your responses.\n",
    "    Here is the memory (it may be empty): {info}\"\"\"\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Format the chat history for the Trustcall extractor\n",
    "    system_msg = \"\"\"Reflect on following interaction. Use the provided tools to retain any necessary memories about the user. \n",
    "    Use parallel tool calling to handle updates & insertions simultaneously:\"\"\"\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=system_msg)] + state[\"messages\"]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \n",
    "                                        \"existing\": existing_memories})\n",
    "\n",
    "    # Save save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "\n",
    "# Conditional edge\n",
    "def route_message(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the memories and chat history to decide whether to update the memory collection.\"\"\"\n",
    "\n",
    "    # Get the user ID from the config# Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"[{mem.key}]: {mem.value}\" for mem in memories)\n",
    "\n",
    "    # Consider whether to save memories\n",
    "    model_with_structure = model.with_structured_output(SaveMemory)\n",
    "    system_msg = f\"\"\"You manage are deciding whether to update the memory collection for the user.\n",
    "    Here is the current memory collection (it may be empty): <memories>{info}</memories>\n",
    "    Here is the chat history. Assess whether the chat history contains any information that should be added to the memory collection.\"\"\"\n",
    "    store_memories_flag = model_with_structure.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    # Check if model has chosen to store memories\n",
    "    if store_memories_flag.store_memories:\n",
    "        return \"write_memory\"\n",
    "    \n",
    "    # Otherwise, finish; user can send the next message\n",
    "    return END\n",
    "\n",
    "# Create the graph + all nodes\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define the flow of the memory extraction process\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(\"__start__\", \"call_model\")\n",
    "builder.add_node(write_memory)\n",
    "builder.add_conditional_edges(\"call_model\", route_message, [\"write_memory\", END])\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# We compile the graph with the checkpointer and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Lance! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"xxx\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a lot of fun, Lance! San Francisco has some great biking routes. Do you have a favorite trail or area you like to explore?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Yes, I went to the Marin headlands today.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds amazing, Lance! The Marin Headlands offer some stunning views and great trails. How was your ride today?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Yes, I went to the Marin headlands today.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the agent decides to save the memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like Miwok trail. Also, I like bakeries after biking.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Miwok Trail is a fantastic choice with its beautiful scenery! And treating yourself to a bakery visit after a ride sounds like a perfect way to end the day. Do you have a favorite bakery you like to visit?\n"
     ]
    }
   ],
   "source": [
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"I like Miwok trail. Also, I like bakeries after biking.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"User's name is Lance.\", 'context': 'User introduced themselves.'}\n",
      "{'content': 'User likes to bike around San Francisco.', 'context': 'User shared their interest in biking.'}\n",
      "{'content': 'User went biking in the Marin Headlands and likes the Miwok trail. Also, they enjoy visiting bakeries after biking.', 'context': 'User shared their recent biking experience.'}\n",
      "{'content': 'User likes the Miwok trail and enjoys visiting bakeries after biking.', 'context': 'User shared their preferences for biking trails and post-biking activities.'}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "\n",
    "# Search \n",
    "for memory in across_thread_memory.search((\"memories\", user_id)):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new thread and chat with the chatbot again.\n",
    "\n",
    "It should retain the memories from the previous thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What bakeries would you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! Since you enjoy visiting bakeries after biking, especially around the San Francisco area, here are a few recommendations you might like:\n",
      "\n",
      "1. **Tartine Bakery** - Known for its delicious bread and pastries, it's a must-visit in the Mission District.\n",
      "2. **Arsicault Bakery** - Famous for its croissants, this place in the Richmond District is worth a stop.\n",
      "3. **B. Patisserie** - Located in Lower Pacific Heights, it offers a delightful selection of pastries and cakes.\n",
      "4. **Craftsman and Wolves** - A contemporary patisserie in the Mission District with unique and creative offerings.\n",
      "5. **Noe Valley Bakery** - A cozy spot in Noe Valley with a great selection of baked goods.\n",
      "\n",
      "These spots should be perfect for a post-biking treat!\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"yyy\", \"user_id\": \"1\"}}\n",
    "\n",
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"What bakeries would you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/a436394f-e565-4377-b0a7-045257cfc69b/r\n",
    "\n",
    "## TODO: Add Template screenshots and closing thoughts \n",
    "\n",
    "https://github.com/langchain-ai/memory-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
