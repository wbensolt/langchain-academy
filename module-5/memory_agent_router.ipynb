{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Agent\n",
    "\n",
    "## Review\n",
    "\n",
    "We created a chatbot that saves semantic memories to a single [user profile](https://langchain-ai.github.io/langgraph/concepts/memory/#profile) or [collection](https://langchain-ai.github.io/langgraph/concepts/memory/#collection).\n",
    "\n",
    "We introduced [Trustcall](https://github.com/hinthornw/trustcall) as a way to update either schema.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going to turn our chatbot into a simple [agent](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/).\n",
    "\n",
    "The chatbot *always* reflected on the conversation and saved memories. \n",
    "\n",
    "The central difference is that the agent will decide *when* to save memories. \n",
    "\n",
    "For this, we're going to introduce a tool that allows the agent to save memories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Check if the variable is set in the OS environment\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # If not set, prompt the user for input\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "    \n",
    "    # Set the environment variable for the current process\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a router\n",
    "\n",
    "There are many different [agent](https://langchain-ai.github.io/langgraph/concepts/high_level/) architectures to choose from.\n",
    "\n",
    "Here, we'll implement something simple: a router that decides when to save memories.\n",
    "\n",
    "This will reflect on the chat history and any prior memories to decide whether to update the memory collection.\n",
    "\n",
    "The updating itself will be handled by `Trustcall`, as before!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Schema for binary decision to save memories\n",
    "class SaveMemory(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    store_memories: bool = Field(description=\"Decision to save memories based on the conversation with the user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's chat model [chat model](https://python.langchain.com/docs/concepts/chat_models/) interface has a [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) method to enforce structured output.\n",
    "\n",
    "As we showed before, this is useful when we want to enforce that the output conforms to a schema, and it parses the output for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Bind the schema to the model\n",
    "model_with_structure = model.with_structured_output(SaveMemory)\n",
    "\n",
    "# Current memory collection\n",
    "info = \"This user's name is Lance and he likes to bike.\"\n",
    "\n",
    "# System message\n",
    "system_msg = f\"\"\"You manage are deciding whether to update the memory collection for the user.\n",
    "    Here is the current memory collection (it may be empty): <memories>{info}</memories>\n",
    "    Here is the chat history. Assess whether the chat history contains any information that should be added to the memory collection.\"\"\"\n",
    "\n",
    "# Invoke with new information\n",
    "store_memories_flag = model_with_structure.invoke([SystemMessage(content=system_msg)]+[HumanMessage(content=\"I like to eat croissants\")])\n",
    "store_memories_flag.store_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph definition \n",
    "\n",
    "We add a simple router, `route_message`, that makes a binary decision to save memories.\n",
    "\n",
    "The memory collection updating is handled by `Trustcall` in the `write_memory` node, as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from trustcall import create_extractor\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import ToolMessage, AnyMessage\n",
    "from langchain_core.messages import merge_message_runs\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "    \n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Schema \n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "    context: str = Field(description=\"Additional context for the memory. For example: This was mentioned while discussing career options in Europe.\")\n",
    "\n",
    "# Create the Trustcall extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\",\n",
    "    # This allows the extractor to insert new memories\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "# Node definitions\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"[{mem.key}]: {mem.value}\" for mem in memories)\n",
    "    system_msg = f\"\"\"You are a helpful assistant with memory that provides information about the user.  \n",
    "    If you have memory for this user, use it to personalize your responses.\n",
    "    Here is the memory (it may be empty): {info}\"\"\"\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the most recent memories for context\n",
    "    existing_items = store.search(namespace)\n",
    "\n",
    "    # Format the existing memories for the Trustcall extractor\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
    "                          for existing_item in existing_items]\n",
    "                          if existing_items\n",
    "                          else None\n",
    "                        )\n",
    "\n",
    "    # Format the chat history for the Trustcall extractor\n",
    "    system_msg = \"\"\"Reflect on following interaction. Use the provided tools to retain any necessary memories about the user. \n",
    "    Use parallel tool calling to handle updates & insertions simultaneously:\"\"\"\n",
    "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=system_msg)] + state[\"messages\"]))\n",
    "\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": updated_messages, \n",
    "                                        \"existing\": existing_memories})\n",
    "\n",
    "    # Save save the memories from Trustcall to the store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace,\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "            )\n",
    "\n",
    "# Conditional edge\n",
    "def route_message(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the memories and chat history to decide whether to update the memory collection.\"\"\"\n",
    "\n",
    "    # Get the user ID from the config# Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    info = \"\\n\".join(f\"[{mem.key}]: {mem.value}\" for mem in memories)\n",
    "\n",
    "    # Consider whether to save memories\n",
    "    model_with_structure = model.with_structured_output(SaveMemory)\n",
    "    system_msg = f\"\"\"You manage are deciding whether to update the memory collection for the user.\n",
    "    Here is the current memory collection (it may be empty): <memories>{info}</memories>\n",
    "    Here is the chat history. Assess whether the chat history contains any information that should be added to the memory collection.\"\"\"\n",
    "    store_memories_flag = model_with_structure.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    # Check if model has chosen to store memories\n",
    "    if store_memories_flag.store_memories:\n",
    "        return \"write_memory\"\n",
    "    \n",
    "    # Otherwise call model \n",
    "    return \"call_model\"\n",
    "\n",
    "# Create the graph + all nodes\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define the flow of the memory extraction process\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(write_memory)\n",
    "builder.add_conditional_edges(\"__start__\", route_message, [\"write_memory\", \"call_model\"])\n",
    "builder.add_edge(\"write_memory\", \"call_model\")\n",
    "builder.add_edge(\"call_model\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# We compile the graph with the checkpointer and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Lance! Welcome back. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to bike around San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great, Lance! Biking around San Francisco must be a fantastic way to explore the city. Do you have any favorite routes or spots you like to visit while biking?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Yes, I went to the Marin headlands today.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like an amazing ride, Lance! The Marin Headlands offer some stunning views. How was your ride today? Did you enjoy the scenery?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Yes, I went to the Marin headlands today.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue chatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like Miwok trail. Also, I like bakeries after biking.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Miwok Trail is a beautiful choice, and treating yourself to a visit to a bakery afterward sounds like a perfect way to end a ride. Do you have a favorite bakery you like to visit after biking?\n"
     ]
    }
   ],
   "source": [
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"I like Miwok trail. Also, I like bakeries after biking.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"User's name is Lance.\", 'context': 'User introduced themselves.'}\n",
      "{'content': 'User likes to bike around San Francisco.', 'context': 'User shared their interest in biking.'}\n",
      "{'content': 'User biked to the Marin Headlands and likes Miwok trail. They also enjoy visiting bakeries after biking.', 'context': 'User shared their recent biking experience in San Francisco.'}\n",
      "{'content': 'User likes Miwok trail and enjoys visiting bakeries after biking.', 'context': 'User shared their favorite trail and post-biking activity.'}\n"
     ]
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "\n",
    "# Search \n",
    "for memory in across_thread_memory.search((\"memories\", user_id)):\n",
    "    print(memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new thread and chat with the chatbot again.\n",
    "\n",
    "It should retain the memories from the previous thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What bakeries would you recommend for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have any specific information about your location or preferences, so I can offer some general recommendations. If you're looking for well-known bakeries, you might want to try:\n",
      "\n",
      "1. **Tartine Bakery** - Known for its artisanal bread and pastries, located in San Francisco.\n",
      "2. **Levain Bakery** - Famous for its cookies, located in New York City.\n",
      "3. **Boudin Bakery** - Known for its sourdough bread, located in San Francisco.\n",
      "4. **Magnolia Bakery** - Famous for its cupcakes, located in New York City.\n",
      "\n",
      "If you can provide more details about your location or what you're looking for in a bakery, I can offer more tailored suggestions!\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# Chat with the chatbot\n",
    "input_messages = [HumanMessage(content=\"What bakeries would you recommend for me?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/a436394f-e565-4377-b0a7-045257cfc69b/r\n",
    "\n",
    "## TODO: Add Template screenshots and closing thoughts \n",
    "\n",
    "https://github.com/langchain-ai/memory-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
