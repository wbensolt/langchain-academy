{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Profile Schema \n",
    "\n",
    "## Review\n",
    "\n",
    "We introduced the [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) as a way to save and retrieve long-term memories.\n",
    "\n",
    "We built a simple chatbot that uses both short-term (within-thread) and long-term (across-thread) memory.\n",
    "\n",
    "This saved long-term semantic (facts about the user) memories [\"in the hot path\"](https://langchain-ai.github.io/langgraph/concepts/memory/#writing-memories-in-the-hot-path), as the user is chatting with it.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Our chatbot saved memories to the [store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) in the simplest possible way. \n",
    "\n",
    "First, it wrote memories as a string that simply overwrites the previous memory. In practice, we often want memories to have a particular structure, and ensure that there is no information loss over time. \n",
    "\n",
    "Second, it wrote based upon a user-provided keyword (`remember`). In practice, we often want the chatbot to decide when to save a memory rather than relying on the user to explicitly tell it. \n",
    "\n",
    "Let's address these shortcomings, starting here with the first.\n",
    " \n",
    "Here, we'll use a schema to save memories and show a useful library, [Trustcall](https://github.com/hinthornw/trustcall), for updating schemas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_openai langgraph trustcall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # Check if the variable is set in the OS environment\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # If not set, prompt the user for input\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "    \n",
    "    # Set the environment variable for the current process\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a memory schema\n",
    "\n",
    "The [LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) supports any schema for the `value` part of the key-value pair. \n",
    "\n",
    "This allows you to store structured information in the memory store. \n",
    "\n",
    "Python has many different types for [structured data](https://python.langchain.com/docs/concepts/structured_outputs/#schema-definition). \n",
    "\n",
    "### JSON Schema\n",
    "\n",
    "The simplest and most common format for structured output is a JSON-like structure.\n",
    "\n",
    "In Python, this can be represented as a dictionary (dict) or list (list). \n",
    "\n",
    "JSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema\n",
    "simple_user_schema = {\n",
    "\"type\": \"object\",\n",
    "\"properties\": {\n",
    "    \"content\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The main content of the memory. For example: User expressed interest in learning about French.\"\n",
    "    },\n",
    "    \"context\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\"type\": \"string\"},\n",
    "        \"description\": \"Additional context for the memory. For example: This was mentioned while discussing career options in Europe.\",\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each field has a `type` and a `description`.\n",
    "\n",
    "We can create an instance of this schema and save it to the memory store.\n",
    "\n",
    "### Pydantic Schema\n",
    "\n",
    "As a second example, [Pydantic](https://docs.pydantic.dev/latest/) is particularly useful for defining structured output schemas.\n",
    "\n",
    "It offers type hints and validation. Here's an example of a Pydantic schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Schema\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "    context: str = Field(description=\"Additional context for the memory. For example: This was mentioned while discussing career options in Europe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a schema to the store\n",
    "\n",
    "Let's save an instance of this schema to the memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema\n",
    "user_instance_json = {\n",
    "    \"content\": \"I'm Lance\",\n",
    "    \"context\": \"Mentioned as an introduction to the chat.\"\n",
    "}\n",
    "\n",
    "# Pydantic Schema\n",
    "user_instance_pydantic = Memory(content=\"I'm Lance\", \n",
    "                                context=\"Mentioned as an introduction to the chat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) method to save an object to the store by namespace and key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = str(uuid.uuid4())\n",
    "value = user_instance_json\n",
    "in_memory_store.put(namespace_for_memory, key, value)\n",
    "\n",
    "key = str(uuid.uuid4())\n",
    "value = user_instance_pydantic\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) to retrieve objects from the store by namespace.\n",
    "\n",
    "We can see that the different schemas are saved their own keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': {'content': \"I'm Lance\", 'context': 'Mentioned as an introduction to the chat.'}, 'key': '7022fd13-eaa2-4503-a96e-c293daa64eea', 'namespace': ['1', 'memories'], 'created_at': '2024-10-25T20:40:11.323604+00:00', 'updated_at': '2024-10-25T20:40:11.323607+00:00'}\n",
      "{'value': Memory(content=\"I'm Lance\", context='Mentioned as an introduction to the chat.'), 'key': '8258e862-37ef-445d-8ed1-433586439ad9', 'namespace': ['1', 'memories'], 'created_at': '2024-10-25T20:40:11.323699+00:00', 'updated_at': '2024-10-25T20:40:11.323700+00:00'}\n"
     ]
    }
   ],
   "source": [
    "# Search \n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing schemas\n",
    "\n",
    "The concept of [tool calling](https://python.langchain.com/docs/concepts/tool_calling/) allows models to write [structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/#schema-definition) (e.g., a schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Bind Memory schema as a tool to the model\n",
    "model_with_tools = model.bind_tools([Memory])\n",
    "\n",
    "# Invoke the model\n",
    "output = model_with_tools.invoke([HumanMessage(\"Hi!.\")])\n",
    "output.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Memory (call_f4uDHsj1uFNLN14x3GYgJzzy)\n",
      " Call ID: call_f4uDHsj1uFNLN14x3GYgJzzy\n",
      "  Args:\n",
      "    content: User's name is Lance and he likes to bike.\n",
      "    context: User introduced himself and shared his interest in biking.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with personal information, which should trigger the tool call\n",
    "output = model_with_tools.invoke([HumanMessage(\"My name is Lance, I like to bike.\")])\n",
    "output.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"User's name is Lance and he likes to bike.\",\n",
       " 'context': 'User introduced himself and shared his interest in biking.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error handling and updating\n",
    "\n",
    "While models can be very good at extracting information from unstructured text, [they can also make mistakes](https://github.com/hinthornw/trustcall?tab=readme-ov-file#complex-schema).\n",
    "\n",
    "While working on memory over the past few months, we've experienced these problems first-hand: \n",
    "\n",
    "1. Extraction can become error-prone as the schema gets more complex.\n",
    "2. Information loss can occur when updating a schema, especially if the schema contains a lot of information or has a complex structure.\n",
    "\n",
    "One of the engineers on the LangChain team, [Will Fu-Hinthorn](https://github.com/hinthornw) created the [TrustCall](https://trustcall.langchain.com/) library to help address these problems.\n",
    "\n",
    "Let's show an example, starting with a simple schema for a user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    user_location: str = Field(description=\"The user's location\")\n",
    "    interests: list = Field(description=\"A list of the user's interests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a conversation that we want to use to create the user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\"User: Hi, I'm Lance.\n",
    "Assistant: Nice to meet you, Lance.\n",
    "User: I really like biking around San Francisco.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an extractor for this schema with TrustCall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserProfile(user_name='Lance', user_location='San Francisco', interests=['biking'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\",\n",
    ")\n",
    "\n",
    "# Instruction\n",
    "instruction = f\"\"\"Extract the user profile from the following conversation:\n",
    "<convo>\n",
    "{conversation}\n",
    "</convo>\"\"\"\n",
    "\n",
    "# Invoke the extractor\n",
    "result = trustcall_extractor.invoke({\"messages\": [HumanMessage(content=instruction)]})\n",
    "existing_profile = result[\"responses\"][0]\n",
    "existing_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance',\n",
       " 'user_location': 'San Francisco',\n",
       " 'interests': ['biking']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_profile.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One primary benefit of Trustcall is that it will seamlessly check schema validation and handle re-tries for you.\n",
    "\n",
    "The central intuition behind Trustcall is that it prompts the model to produce [JSON Patch](https://jsonpatch.com/) to correct schema validation errors.\n",
    "\n",
    "This is both more reliable than naive re-prompting and cheaper since you only regenerate a subset of the full schema.\n",
    "\n",
    "JSON patches are *also* address the problem of information loss when updating a schema! \n",
    "\n",
    "Rather than naively re-generating the full output, it prompts the LLM to generate a concise patch to update the schema.\n",
    "\n",
    "Let's see updating in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserProfile(user_name='Lance', user_location='San Francisco', interests=['biking', 'going to a bakery'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the conversation\n",
    "updated_conversation = \"\"\"User: Hi, I'm Lance.\n",
    "Assistant: Nice to meet you, Lance.\n",
    "User: I really like biking around San Francisco.\n",
    "Assistant: San Francisco is a great city! Where do you go after biking?\n",
    "User: I really like to go to a bakery.\"\"\"\n",
    "\n",
    "# Update the instruction\n",
    "updated_instruction = f\"\"\"Update the memory (JSON doc) to incorporate new information from the following conversation:\n",
    "<convo>\n",
    "{updated_conversation}\n",
    "</convo>\"\"\"\n",
    "\n",
    "# Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)\n",
    "result = trustcall_extractor.invoke({\"messages\": [HumanMessage(content=updated_instruction)], \n",
    "                                     \"existing\": {\"UserProfile\": existing_profile.model_dump()}})\n",
    "updated_profile = result[\"responses\"][0]\n",
    "updated_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Lance',\n",
       " 'user_location': 'San Francisco',\n",
       " 'interests': ['biking', 'going to a bakery']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_profile.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot that uses TrustCall to format and update memories\n",
    "\n",
    "Now, let's use the store in a simple chatbot that saves memories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Define the namespace for the memories\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    # Retrieve the profile from the store using the key that we save the profile under\n",
    "    profile = store.get(namespace, \"user_profile\")\n",
    "\n",
    "    # Get the last message from the user \n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Check if it contains the keyword \"remember\"\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        \n",
    "        # Get the profile as the value from the list, and convert it to a JSON doc\n",
    "        existing_profile = {\"UserProfile\": profile.value} if profile else None\n",
    "\n",
    "        # Instructions \n",
    "        instruction = f\"\"\"Create or update the memory (JSON doc) to incorporate information from the following conversation:\n",
    "        <convo>\n",
    "        {list(state['messages'])}\n",
    "        </convo>\"\"\"\n",
    "        \n",
    "        # Invoke the extractor\n",
    "        result = trustcall_extractor.invoke({\"messages\": [HumanMessage(content=instruction)], \"existing\": existing_profile})\n",
    "        \n",
    "        # Get the updated profile\n",
    "        updated_profile = result[\"responses\"][0].model_dump(mode=\"json\")\n",
    "\n",
    "        # Save the updated profile\n",
    "        key = \"user_profile\"\n",
    "        store.put(namespace, key, {\"profile\": updated_profile})\n",
    "\n",
    "    # Format all memories for the system prompt\n",
    "    system_msg = f\"You are a helpful assistant. Here is relevant information about the user: {profile.value if profile else None}\"\n",
    "    \n",
    "    # Invoke the model with the system prompt that contains the memories as well as the user's messages\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", END)\n",
    "\n",
    "# Store from cross-thread memory\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# We compile the graph with the checkpointer and store\n",
    "graph = builder.compile(checkpointer=MemorySaver(), store=in_memory_store)\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! Remember: my name is Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Lance! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a user ID for across-thread memory as well as a thread ID for within-thread memory\n",
    "config = {\"configurable\": {\"thread_id\": \"4\", \"user_id\": \"1\"}}\n",
    "\n",
    "# I give the chatbot a message to remember using the keyword \"remember\"\n",
    "input_messages = [HumanMessage(content=\"Hi! Remember: my name is Lance\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember: I like to bike\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Got it, Lance! You like to bike. Do you have any biking-related questions or topics you'd like to discuss?\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "input_messages = [HumanMessage(content=\"Remember: I like to bike\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'profile': {'user_name': 'Lance',\n",
       "   'user_location': '',\n",
       "   'interests': ['bike']}},\n",
       " 'key': 'user_profile',\n",
       " 'namespace': ['memories', '1'],\n",
       " 'created_at': '2024-10-25T22:48:46.305930+00:00',\n",
       " 'updated_at': '2024-10-25T22:48:53.747877+00:00'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the namespace for the memories\n",
    "namespace = (\"memories\", \"1\")\n",
    "\n",
    "# Retrieve the profile from the store using the key that we save the profile under\n",
    "profile = in_memory_store.get(namespace, \"user_profile\")\n",
    "profile.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Remember: I live in San Francisco\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Sure thing, Lance! You live in San Francisco. If you need any information or recommendations related to biking in the area, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation\n",
    "input_messages = [HumanMessage(content=\"Remember: I live in San Francisco\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {'profile': {'user_name': 'Lance',\n",
       "   'user_location': 'San Francisco',\n",
       "   'interests': ['bike']}},\n",
       " 'key': 'user_profile',\n",
       " 'namespace': ['memories', '1'],\n",
       " 'created_at': '2024-10-25T22:48:46.305930+00:00',\n",
       " 'updated_at': '2024-10-25T22:51:02.175359+00:00'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the namespace for the memories\n",
    "namespace = (\"memories\", \"1\")\n",
    "\n",
    "# Retrieve the profile from the store using the key that we save the profile under\n",
    "profile = in_memory_store.get(namespace, \"user_profile\")\n",
    "profile.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the profile is being updated as we continue the conversation!\n",
    "\n",
    "## LangSmith \n",
    "\n",
    "We can view the chatbot's memory updates in LangSmith, and in particular see how TrustCall is used to update the memory.\n",
    "\n",
    "https://smith.langchain.com/public/c422bbba-1320-4df6-853a-e654ca0ef5c9/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
